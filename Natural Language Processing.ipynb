{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f106e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b8d5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Bonjour, comment ca va ?     ',\n",
       " '    Heyyyyy, how are you doing ?   ',\n",
       " '        Hallo, wie gehts ?     ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['   Bonjour, comment ca va ?     ',\n",
    "         '    Heyyyyy, how are you doing ?   ',\n",
    "         '        Hallo, wie gehts ?     ']\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48f66fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour, comment ca va ?',\n",
       " 'Heyyyyy, how are you doing ?',\n",
       " 'Hallo, wie gehts ?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffcfbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcd Who is abcd ? That's not a real name!!! abcd\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043aae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Who is abcd ? That's not a real name!!! \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.strip('bdac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a59739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love koalas, koalas are the cuttest animals on Earth.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love koalas, koalas are the cuttest animals on Earth.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8af599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love pandas, pandas are the cuttest animals on Earth.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"koala\",\"panda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83cd20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"alicia keys / bts /justin bieber\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd12dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alicia keys ', ' bts ', 'justin bieber']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb5691cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f73711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love football so much. football is my passion. who else loves football ?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc4158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef01091b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like  minutes, this is ridiculous'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f794e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8070623a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a180b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea OMG so tasty channel XOXO   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '') \n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54e4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"   I LOVE Pizza 999 @^_^\", \n",
    "             \"  Le Wagon is amazing, take care - 666\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b92bacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') \n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd70962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love pizza', 'le wagon is amazing take care']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences = [basic_cleaning(sentence) for sentence in sentences]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3395b3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Le Wagon!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"<head><body>Hello Le Wagon!</body></head>\"\"\"\n",
    "cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "552d9e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['darkvador@gmail.com', 'batman@outlook.com']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = 'This is a random text, authored by darkvador@gmail.com and batman@outlook.com, WOW!'\n",
    "\n",
    "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f1c49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is during our darkest moments that we must focus to see the light'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"It is during our darkest moments that we must focus to see the light\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0faba437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcd',\n",
       " 'Who',\n",
       " 'is',\n",
       " 'abcd',\n",
       " '?',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'a',\n",
       " 'real',\n",
       " 'name',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'abcd']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3d8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jinru/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a179f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) # you can also choose other languages\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2eca6b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea58c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('chinese')) # you can also choose other languages\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3af9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即或',\n",
       " '即若',\n",
       " '却不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她的',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " '应该',\n",
       " '开外',\n",
       " '开始',\n",
       " '开展',\n",
       " '引起',\n",
       " '强烈',\n",
       " '强调',\n",
       " '归',\n",
       " '当',\n",
       " '当前',\n",
       " '当时',\n",
       " '当然',\n",
       " '当着',\n",
       " '形成',\n",
       " '彻底',\n",
       " '彼',\n",
       " '彼此',\n",
       " '往',\n",
       " '往往',\n",
       " '待',\n",
       " '後来',\n",
       " '後面',\n",
       " '得',\n",
       " '得出',\n",
       " '得到',\n",
       " '心里',\n",
       " '必然',\n",
       " '必要',\n",
       " '必须',\n",
       " '怎',\n",
       " '怎么',\n",
       " '怎么办',\n",
       " '怎么样',\n",
       " '怎样',\n",
       " '怎麽',\n",
       " '总之',\n",
       " '总是',\n",
       " '总的来看',\n",
       " '总的来说',\n",
       " '总的说来',\n",
       " '总结',\n",
       " '总而言之',\n",
       " '恰恰相反',\n",
       " '您',\n",
       " '意思',\n",
       " '愿意',\n",
       " '慢说',\n",
       " '成为',\n",
       " '我',\n",
       " '我们',\n",
       " '我的',\n",
       " '或',\n",
       " '或是',\n",
       " '或者',\n",
       " '战斗',\n",
       " '所',\n",
       " '所以',\n",
       " '所有',\n",
       " '所谓',\n",
       " '打',\n",
       " '扩大',\n",
       " '把',\n",
       " '抑或',\n",
       " '拿',\n",
       " '按',\n",
       " '按照',\n",
       " '换句话说',\n",
       " '换言之',\n",
       " '据',\n",
       " '掌握',\n",
       " '接着',\n",
       " '接著',\n",
       " '故',\n",
       " '故此',\n",
       " '整个',\n",
       " '方便',\n",
       " '方面',\n",
       " '旁人',\n",
       " '无宁',\n",
       " '无法',\n",
       " '无论',\n",
       " '既',\n",
       " '既是',\n",
       " '既然',\n",
       " '时候',\n",
       " '明显',\n",
       " '明确',\n",
       " '是',\n",
       " '是不是',\n",
       " '是否',\n",
       " '是的',\n",
       " '显然',\n",
       " '显著',\n",
       " '普通',\n",
       " '普遍',\n",
       " '更加',\n",
       " '曾经',\n",
       " '替',\n",
       " '最后',\n",
       " '最大',\n",
       " '最好',\n",
       " '最後',\n",
       " '最近',\n",
       " '最高',\n",
       " '有',\n",
       " '有些',\n",
       " '有关',\n",
       " '有利',\n",
       " '有力',\n",
       " '有所',\n",
       " '有效',\n",
       " '有时',\n",
       " '有点',\n",
       " '有的',\n",
       " '有着',\n",
       " '有著',\n",
       " '望',\n",
       " '朝',\n",
       " '朝着',\n",
       " '本',\n",
       " '本着',\n",
       " '来',\n",
       " '来着',\n",
       " '极了',\n",
       " '构成',\n",
       " '果然',\n",
       " '果真',\n",
       " '某',\n",
       " '某个',\n",
       " '某些',\n",
       " '根据',\n",
       " '根本',\n",
       " '欢迎',\n",
       " '正在',\n",
       " '正如',\n",
       " '正常',\n",
       " '此',\n",
       " '此外',\n",
       " '此时',\n",
       " '此间',\n",
       " '毋宁',\n",
       " '每',\n",
       " '每个',\n",
       " '每天',\n",
       " '每年',\n",
       " '每当',\n",
       " '比',\n",
       " '比如',\n",
       " '比方',\n",
       " '比较',\n",
       " '毫不',\n",
       " '没有',\n",
       " '沿',\n",
       " '沿着',\n",
       " '注意',\n",
       " '深入',\n",
       " '清楚',\n",
       " '满足',\n",
       " '漫说',\n",
       " '焉',\n",
       " '然则',\n",
       " '然后',\n",
       " '然後',\n",
       " '然而',\n",
       " '照',\n",
       " '照着',\n",
       " '特别是',\n",
       " '特殊',\n",
       " '特点',\n",
       " '现代',\n",
       " '现在',\n",
       " '甚么',\n",
       " '甚而',\n",
       " '甚至',\n",
       " '用',\n",
       " '由',\n",
       " '由于',\n",
       " '由此可见',\n",
       " '的',\n",
       " '的话',\n",
       " '目前',\n",
       " '直到',\n",
       " '直接',\n",
       " '相似',\n",
       " '相信',\n",
       " '相反',\n",
       " '相同',\n",
       " '相对',\n",
       " '相对而言',\n",
       " '相应',\n",
       " '相当',\n",
       " '相等',\n",
       " '省得',\n",
       " '看出',\n",
       " '看到',\n",
       " '看来',\n",
       " '看看',\n",
       " '看见',\n",
       " '真是',\n",
       " '真正',\n",
       " '着',\n",
       " '着呢',\n",
       " '矣',\n",
       " '知道',\n",
       " '确定',\n",
       " '离',\n",
       " '积极',\n",
       " '移动',\n",
       " '突出',\n",
       " '突然',\n",
       " '立即',\n",
       " '第',\n",
       " '等',\n",
       " '等等',\n",
       " '管',\n",
       " '紧接着',\n",
       " '纵',\n",
       " '纵令',\n",
       " '纵使',\n",
       " '纵然',\n",
       " '练习',\n",
       " '组成',\n",
       " '经',\n",
       " '经常',\n",
       " '经过',\n",
       " '结合',\n",
       " '结果',\n",
       " '给',\n",
       " '绝对',\n",
       " '继续',\n",
       " '继而',\n",
       " '维持',\n",
       " '综上所述',\n",
       " '罢了',\n",
       " '考虑',\n",
       " '者',\n",
       " '而',\n",
       " '而且',\n",
       " '而况',\n",
       " '而外',\n",
       " '而已',\n",
       " '而是',\n",
       " '而言',\n",
       " '联系',\n",
       " '能',\n",
       " '能否',\n",
       " '能够',\n",
       " '腾',\n",
       " '自',\n",
       " '自个儿',\n",
       " '自从',\n",
       " '自各儿',\n",
       " '自家',\n",
       " '自己',\n",
       " '自身',\n",
       " '至',\n",
       " '至于',\n",
       " '良好',\n",
       " '若',\n",
       " '若是',\n",
       " '若非',\n",
       " '范围',\n",
       " '莫若',\n",
       " '获得',\n",
       " '虽',\n",
       " '虽则',\n",
       " '虽然',\n",
       " '虽说',\n",
       " '行为',\n",
       " '行动',\n",
       " '表明',\n",
       " '表示',\n",
       " '被',\n",
       " '要',\n",
       " '要不',\n",
       " '要不是',\n",
       " '要不然',\n",
       " '要么',\n",
       " '要是',\n",
       " '要求',\n",
       " '规定',\n",
       " '觉得',\n",
       " '认为',\n",
       " '认真',\n",
       " '认识',\n",
       " '让',\n",
       " '许多',\n",
       " '论',\n",
       " '设使',\n",
       " '设若',\n",
       " '该',\n",
       " '说明',\n",
       " '诸位',\n",
       " '谁',\n",
       " '谁知',\n",
       " '赶',\n",
       " '起',\n",
       " '起来',\n",
       " '起见',\n",
       " '趁',\n",
       " '趁着',\n",
       " '越是',\n",
       " '跟',\n",
       " '转动',\n",
       " '转变',\n",
       " '转贴',\n",
       " '较',\n",
       " '较之',\n",
       " '边',\n",
       " '达到',\n",
       " '迅速',\n",
       " '过',\n",
       " '过去',\n",
       " '过来',\n",
       " '运用',\n",
       " '还是',\n",
       " '还有',\n",
       " '这',\n",
       " '这个',\n",
       " '这么',\n",
       " '这么些',\n",
       " '这么样',\n",
       " '这么点儿',\n",
       " '这些',\n",
       " '这会儿',\n",
       " '这儿',\n",
       " '这就是说',\n",
       " '这时',\n",
       " '这样',\n",
       " '这点',\n",
       " '这种',\n",
       " '这边',\n",
       " '这里',\n",
       " '这麽',\n",
       " '进入',\n",
       " '进步',\n",
       " '进而',\n",
       " '进行',\n",
       " '连',\n",
       " '连同',\n",
       " '适应',\n",
       " '适当',\n",
       " '适用',\n",
       " '逐步',\n",
       " '逐渐',\n",
       " '通常',\n",
       " '通过',\n",
       " '造成',\n",
       " '遇到',\n",
       " '遭到',\n",
       " '避免',\n",
       " '那',\n",
       " '那个',\n",
       " '那么',\n",
       " '那么些',\n",
       " '那么样',\n",
       " '那些',\n",
       " '那会儿',\n",
       " '那儿',\n",
       " '那时',\n",
       " '那样',\n",
       " '那边',\n",
       " '那里',\n",
       " '那麽',\n",
       " '部分',\n",
       " '鄙人',\n",
       " '采取',\n",
       " '里面',\n",
       " '重大',\n",
       " '重新',\n",
       " '重要',\n",
       " '鉴于',\n",
       " '问题',\n",
       " '防止',\n",
       " '阿',\n",
       " '附近',\n",
       " '限制',\n",
       " '除',\n",
       " '除了',\n",
       " '除此之外',\n",
       " '除非',\n",
       " '随',\n",
       " '随着',\n",
       " '随著',\n",
       " '集中',\n",
       " '需要',\n",
       " '非但',\n",
       " '非常',\n",
       " '非徒',\n",
       " '靠',\n",
       " '顺',\n",
       " '顺着',\n",
       " '首先',\n",
       " '高兴'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea894f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"i\", \"am\", \"not\", \"going\", \"to\", \"go\", \"to\", \"the\", \"club\", \n",
    "          \"and\",\"party\", \"all\", \"night\", \"long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2d8e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'not', 'to', 'to', 'the', 'and', 'all']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_removed = [w for w in tokens if w in stop_words] \n",
    "stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8d81212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'club', 'party', 'night', 'long']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_cleaned = [w for w in tokens if not w in stop_words] \n",
    "tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6bc1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "884b7fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he was running and eating at the same time  he has a bad habit of swimming after playing  hours in the sun'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63654867",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/vncnltvn4gsc7gbvnvg8j3dc0000gn/T/ipykernel_59368/2946694443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = word_tokenize(cleaned_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f3798d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/vncnltvn4gsc7gbvnvg8j3dc0000gn/T/ipykernel_59368/1870254531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_sentence_no_stopword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_sentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_sentence_no_stopword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_sentence_no_stopword = [w for w in tokenized_sentence if not w in stop_words] \n",
    "tokenized_sentence_no_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21957d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sentence_no_stopword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/vncnltvn4gsc7gbvnvg8j3dc0000gn/T/ipykernel_59368/4123601618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Lemmatizing the verbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m verb_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")  # v --> verbs\n\u001b[0;32m----> 4\u001b[0;31m               for word in tokenized_sentence_no_stopword]\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2 - Lemmatizing the nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_sentence_no_stopword' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Lemmatizing the verbs\n",
    "verb_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")  # v --> verbs\n",
    "              for word in tokenized_sentence_no_stopword]\n",
    "\n",
    "# 2 - Lemmatizing the nouns\n",
    "noun_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\")  # n --> nouns\n",
    "              for word in verb_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "327e046a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'eat', 'time', 'bad', 'habit', 'swim', 'play', 'hours', 'sun']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfc308e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'eat', 'time', 'bad', 'habit', 'swim', 'play', 'hour', 'sun']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aed6cbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running',\n",
       " 'eating',\n",
       " 'time',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'swimming',\n",
       " 'playing',\n",
       " 'hours',\n",
       " 'sun']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence_no_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61905345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>verb_lemmatized</th>\n",
       "      <th>noun_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eating</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>habit</td>\n",
       "      <td>habit</td>\n",
       "      <td>habit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swimming</td>\n",
       "      <td>swim</td>\n",
       "      <td>swim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>playing</td>\n",
       "      <td>play</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hours</td>\n",
       "      <td>hours</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "      <td>sun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original verb_lemmatized noun_lemmatized\n",
       "0   running             run             run\n",
       "1    eating             eat             eat\n",
       "2      time            time            time\n",
       "3       bad             bad             bad\n",
       "4     habit           habit           habit\n",
       "5  swimming            swim            swim\n",
       "6   playing            play            play\n",
       "7     hours           hours            hour\n",
       "8       sun             sun             sun"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([tokenized_sentence_no_stopword, verb_lemmatized, noun_lemmatized]).T\n",
    "df = df.rename(columns = {0:'original', 1:'verb_lemmatized', 2: 'noun_lemmatized'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5bf595fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01784ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['the young dog is running with the cat',\n",
    "         'running is good for your health',\n",
    "         'your cat is young',\n",
    "         'young young young young young cat cat cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fdfc4ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "66dc0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'dog', 'for', 'good', 'health', 'is', 'running', 'the',\n",
       "       'with', 'young', 'your'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cef4baab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cat  dog  for  good  health  is  \\\n",
       "the young dog is running with the cat        1    1    0     0       0   1   \n",
       "running is good for your health              0    0    1     1       1   1   \n",
       "your cat is young                            1    0    0     0       0   1   \n",
       "young young young young young cat cat cat    3    0    0     0       0   0   \n",
       "\n",
       "                                           running  the  with  young  your  \n",
       "the young dog is running with the cat            1    2     1      1     0  \n",
       "running is good for your health                  1    0     0      0     1  \n",
       "your cat is young                                0    0     0      1     1  \n",
       "young young young young young cat cat cat        0    0     0      5     0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_texts = pd.DataFrame(X.toarray(), \n",
    "                                columns = count_vectorizer.get_feature_names_out(),\n",
    "                                index = texts)\n",
    "\n",
    "vectorized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "edcaa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea639759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.357056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.281507</td>\n",
       "      <td>0.714112</td>\n",
       "      <td>0.357056</td>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.295980</td>\n",
       "      <td>0.365594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.580622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.514496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857493</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       for      good    health        is   running  \\\n",
       "0  0.227904  0.357056  0.000000  0.000000  0.000000  0.227904  0.281507   \n",
       "1  0.000000  0.000000  0.463709  0.463709  0.463709  0.295980  0.365594   \n",
       "2  0.470063  0.000000  0.000000  0.000000  0.000000  0.470063  0.000000   \n",
       "3  0.514496  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        the      with     young      your  \n",
       "0  0.714112  0.357056  0.227904  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.365594  \n",
       "2  0.000000  0.000000  0.470063  0.580622  \n",
       "3  0.000000  0.000000  0.857493  0.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the TfidfVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Training it on the texts\n",
    "weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n",
    "                 columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "weighted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8a6797d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  dog  for  good  health  is  running  the  with  young  your\n",
       "0    3    1    1     1       1   3        2    1     1      3     2"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency = pd.DataFrame([dfs], columns = count_vectorizer.get_feature_names_out())\n",
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "10a601df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dog  for  good  health  running  \\\n",
       "the young dog is running with the cat        1    0     0       0        1   \n",
       "running is good for your health              0    1     1       1        1   \n",
       "your cat is young                            0    0     0       0        0   \n",
       "young young young young young cat cat cat    0    0     0       0        0   \n",
       "\n",
       "                                           the  with  your  \n",
       "the young dog is running with the cat        2     1     0  \n",
       "running is good for your health              0     0     1  \n",
       "your cat is young                            0     0     1  \n",
       "young young young young young cat cat cat    0     0     0  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer with max_df = 2\n",
    "count_vectorizer = CountVectorizer(max_df = 2) # removing \"cat\", \"is\", \"young\"\n",
    "\n",
    "# Train it\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out(),\n",
    "    index = texts\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32fd673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cat  is  young\n",
       "the young dog is running with the cat        1   1      1\n",
       "running is good for your health              0   1      0\n",
       "your cat is young                            1   1      1\n",
       "young young young young young cat cat cat    3   0      5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer with the 3 most frequent words\n",
    "count_vectorizer = CountVectorizer(max_features = 3)\n",
    "\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "     columns = count_vectorizer.get_feature_names_out(),\n",
    "     index = texts\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "52aa01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_movie = [\"I like the movie but NOT the actors\",\n",
    "                \"I like the actors but NOT the movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "45629515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like the movie but NOT the actors</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like the actors but NOT the movie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     actors  but  like  movie  not  the\n",
       "I like the movie but NOT the actors       1    1     1      1    1    2\n",
       "I like the actors but NOT the movie       1    1     1      1    1    2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the sentences\n",
    "count_vectorizer = CountVectorizer()\n",
    "actors_movie_vectorized = count_vectorizer.fit_transform(actors_movie)\n",
    "\n",
    "# Show the representations in a nice DataFrame\n",
    "actors_movie_vectorized = pd.DataFrame(actors_movie_vectorized.toarray(),\n",
    "                                       columns = count_vectorizer.get_feature_names_out(),\n",
    "                                       index = actors_movie)\n",
    "\n",
    "# Show the vectorized movies\n",
    "actors_movie_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9bf11e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors but</th>\n",
       "      <th>but not</th>\n",
       "      <th>like the</th>\n",
       "      <th>movie but</th>\n",
       "      <th>not the</th>\n",
       "      <th>the actors</th>\n",
       "      <th>the movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like the movie but NOT the actors</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like the actors but NOT the movie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     actors but  but not  like the  movie but  \\\n",
       "I like the movie but NOT the actors           0        1         1          1   \n",
       "I like the actors but NOT the movie           1        1         1          0   \n",
       "\n",
       "                                     not the  the actors  the movie  \n",
       "I like the movie but NOT the actors        1           1          1  \n",
       "I like the actors but NOT the movie        1           1          1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the sentences\n",
    "count_vectorizer_n_gram = CountVectorizer(ngram_range = (2,2))\n",
    "# count_vectorizer_n_gram = CountVectorizer(ngram_range = (2,2), token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\") # BI-GRAMS\n",
    "actors_movie_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(actors_movie)\n",
    "\n",
    "# Show the representations in a nice DataFrame\n",
    "actors_movie_vectorized_n_gram = pd.DataFrame(actors_movie_vectorized_n_gram.toarray(),\n",
    "                                              columns = count_vectorizer_n_gram.get_feature_names_out(),\n",
    "                                              index = actors_movie)\n",
    "\n",
    "# Show the vectorized movies with bigrams\n",
    "actors_movie_vectorized_n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "580b3cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like the movie but NOT the actors', 'I like the actors but NOT the movie']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d205b512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/bayes.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"images/bayes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "87674e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/bayes2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"images/bayes2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75aaf270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"emails.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d11d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f9ee3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0143f7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.76\n",
       "1    0.24\n",
       "Name: spam, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(data[\"spam\"].value_counts(normalize = True),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f67521a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "5723    0\n",
       "5724    0\n",
       "5725    0\n",
       "5726    0\n",
       "5727    0\n",
       "Name: spam, Length: 5728, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1aeccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Feature/Target\n",
    "X = data[\"text\"]\n",
    "y = data[\"spam\"]\n",
    "\n",
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline_naive_bayes = make_pipeline(TfidfVectorizer(), \n",
    "                                     MultinomialNB())\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline_naive_bayes, X, y, cv = 5, scoring = [\"recall\"])\n",
    "average_recall = cv_results[\"test_recall\"].mean()\n",
    "np.round(average_recall,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33e8d04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_naive_bayes.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94b5f4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_naive_bayes.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baaba54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Score = 0.9524932488436137\n",
      "Best params = {'multinomialnb__alpha': 0.1, 'tfidfvectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'tfidfvectorizer__ngram_range': ((1,1), (2,2)),\n",
    "    'multinomialnb__alpha': (0.1,1),}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline_naive_bayes, parameters, scoring = \"recall\",\n",
    "                           cv = 5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(data.text,data.spam)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53f47b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71526b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c1ee935",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame(['like mangos oranges', 'frog turtle live ponds', 'kitten puppies fluffy', 'spinach kiwi smoothie', 'kitten love strawberries'], columns = ['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f5f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like mangos oranges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frog turtle live ponds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitten puppies fluffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spinach kiwi smoothie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kitten love strawberries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  documents\n",
       "0       like mangos oranges\n",
       "1    frog turtle live ponds\n",
       "2     kitten puppies fluffy\n",
       "3     spinach kiwi smoothie\n",
       "4  kitten love strawberries"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b978c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         like mangos oranges\n",
       "1      frog turtle live ponds\n",
       "2       kitten puppies fluffy\n",
       "3       spinach kiwi smoothie\n",
       "4    kitten love strawberries\n",
       "Name: documents, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_documents = documents[\"documents\"].apply(cleaning)\n",
    "cleaned_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7466eb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluffy</th>\n",
       "      <th>frog</th>\n",
       "      <th>kitten</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>like</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>mangos</th>\n",
       "      <th>oranges</th>\n",
       "      <th>ponds</th>\n",
       "      <th>puppies</th>\n",
       "      <th>smoothie</th>\n",
       "      <th>spinach</th>\n",
       "      <th>strawberries</th>\n",
       "      <th>turtle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fluffy  frog    kitten     kiwi     like  live      love   mangos  \\\n",
       "0  0.000000   0.0  0.000000  0.00000  0.57735   0.0  0.000000  0.57735   \n",
       "1  0.000000   0.5  0.000000  0.00000  0.00000   0.5  0.000000  0.00000   \n",
       "2  0.614189   0.0  0.495524  0.00000  0.00000   0.0  0.000000  0.00000   \n",
       "3  0.000000   0.0  0.000000  0.57735  0.00000   0.0  0.000000  0.00000   \n",
       "4  0.000000   0.0  0.495524  0.00000  0.00000   0.0  0.614189  0.00000   \n",
       "\n",
       "   oranges  ponds   puppies  smoothie  spinach  strawberries  turtle  \n",
       "0  0.57735    0.0  0.000000   0.00000  0.00000      0.000000     0.0  \n",
       "1  0.00000    0.5  0.000000   0.00000  0.00000      0.000000     0.5  \n",
       "2  0.00000    0.0  0.614189   0.00000  0.00000      0.000000     0.0  \n",
       "3  0.00000    0.0  0.000000   0.57735  0.57735      0.000000     0.0  \n",
       "4  0.00000    0.0  0.000000   0.00000  0.00000      0.614189     0.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_documents = vectorizer.fit_transform(cleaned_documents)\n",
    "vectorized_documents = pd.DataFrame(vectorized_documents.toarray(), \n",
    "                                    columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "vectorized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b14ef64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(max_iter=100, n_components=2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate the LDA \n",
    "n_components = 2\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components, max_iter = 100)\n",
    "\n",
    "# Fit the LDA on the vectorized documents\n",
    "lda_model.fit(vectorized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aca7b9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.211863</td>\n",
       "      <td>0.788137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197517</td>\n",
       "      <td>0.802483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206331</td>\n",
       "      <td>0.793669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.808432</td>\n",
       "      <td>0.191568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.206333</td>\n",
       "      <td>0.793667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.211863  0.788137\n",
       "1  0.197517  0.802483\n",
       "2  0.206331  0.793669\n",
       "3  0.808432  0.191568\n",
       "4  0.206333  0.793667"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_mixture = lda_model.transform(vectorized_documents)\n",
    "pd.DataFrame(document_topic_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55691e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_mixture = pd.DataFrame(lda_model.components_, \n",
    "                                 columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbd5f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluffy</th>\n",
       "      <th>frog</th>\n",
       "      <th>kitten</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>like</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>mangos</th>\n",
       "      <th>oranges</th>\n",
       "      <th>ponds</th>\n",
       "      <th>puppies</th>\n",
       "      <th>smoothie</th>\n",
       "      <th>spinach</th>\n",
       "      <th>strawberries</th>\n",
       "      <th>turtle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524448</td>\n",
       "      <td>0.52308</td>\n",
       "      <td>0.525891</td>\n",
       "      <td>1.069574</td>\n",
       "      <td>0.526204</td>\n",
       "      <td>0.52308</td>\n",
       "      <td>0.524457</td>\n",
       "      <td>0.526204</td>\n",
       "      <td>0.526204</td>\n",
       "      <td>0.52308</td>\n",
       "      <td>0.524448</td>\n",
       "      <td>1.069574</td>\n",
       "      <td>1.069574</td>\n",
       "      <td>0.524457</td>\n",
       "      <td>0.52308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.089741</td>\n",
       "      <td>0.97692</td>\n",
       "      <td>1.465156</td>\n",
       "      <td>0.507776</td>\n",
       "      <td>1.051147</td>\n",
       "      <td>0.97692</td>\n",
       "      <td>1.089732</td>\n",
       "      <td>1.051147</td>\n",
       "      <td>1.051147</td>\n",
       "      <td>0.97692</td>\n",
       "      <td>1.089741</td>\n",
       "      <td>0.507776</td>\n",
       "      <td>0.507776</td>\n",
       "      <td>1.089732</td>\n",
       "      <td>0.97692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fluffy     frog    kitten      kiwi      like     live      love  \\\n",
       "0  0.524448  0.52308  0.525891  1.069574  0.526204  0.52308  0.524457   \n",
       "1  1.089741  0.97692  1.465156  0.507776  1.051147  0.97692  1.089732   \n",
       "\n",
       "     mangos   oranges    ponds   puppies  smoothie   spinach  strawberries  \\\n",
       "0  0.526204  0.526204  0.52308  0.524448  1.069574  1.069574      0.524457   \n",
       "1  1.051147  1.051147  0.97692  1.089741  0.507776  0.507776      1.089732   \n",
       "\n",
       "    turtle  \n",
       "0  0.52308  \n",
       "1  0.97692  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d28a82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(lda_model, vectorizer, top_words):\n",
    "    # 1. TOPIC MIXTURE OF WORDS FOR EACH TOPIC\n",
    "    topic_mixture = pd.DataFrame(lda_model.components_,\n",
    "                                 columns = vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # 2. FINDING THE TOP WORDS FOR EACH TOPIC\n",
    "    ## Number of topics\n",
    "    n_components = topic_mixture.shape[0]\n",
    "    ## Top words for each topic\n",
    "    for topic in range(n_components):\n",
    "        print(\"-\"*10)\n",
    "        print(f\"For topic {topic}, here are the the top {top_words} words with weights:\")\n",
    "        topic_df = topic_mixture.iloc[topic]\\\n",
    "                             .sort_values(ascending = True).head(top_words)\n",
    "        \n",
    "        print(round(topic_df,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "badb4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "For topic 0, here are the the top 5 words with weights:\n",
      "frog      0.523\n",
      "live      0.523\n",
      "ponds     0.523\n",
      "turtle    0.523\n",
      "fluffy    0.524\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "For topic 1, here are the the top 5 words with weights:\n",
      "kiwi        0.508\n",
      "smoothie    0.508\n",
      "spinach     0.508\n",
      "frog        0.977\n",
      "live        0.977\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, vectorizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
